# Week 03 Day 03

- 2023년 11월 22일 (수)

---

## Insights

1) [DL Basic] Long Short-Term Memory, LSTM

    - 바닐라 RNN의 한계
        - The problem of Long-Term Dependencies (장기 의존성 문제)
        
    - LSTM(Long Short-Term Memory) 구조
        - hidden state
        - cell state
        - input gate
        - forget gate
        - output gate
       
    - Gated Recurrent Unit (GRU)
    
    > GRU는 LSTM의 게이트 메커니즘을 유지하면서, 은닉 상태를 업데이트하는 계산을 줄였습니다.


2) [DL Basic] [DL Basic] Transformer
    - Sequential Model
    
    - Transformer
        - Self-Attention
        - Query, Key, Value vectors
        - Positional Encoding
        
    - Scaled Dot-Product Attention (SDPA)
        - 구현: ScaledDotProductAttention 클래스
        
    - Multi-Headed Attention (MHA)
    
    > RNN의 고질적인 문제인 기울기 소실(vanishing gradient) 문제를 해결하기위한 방식으로, 문장 내의 단어 간의 의존성(관계성)을 고려
   

---

## Peer session

- Daily-Code-Challenge

    - LV1.문자열 내 p와 y의 개수
    - LV2.튜플 

---

## Wrap-up

- Transformer 어렵지만 제대로 알고 넘어가야한다. 논문리뷰 같은거 까지 천천히 파보자

- 순 공부 시간 체크

- 루틴 만들기
    - 1 day 1 update
    - 식사 후 산책
    - 도핑
    - 17:30 ~ 19:00 
    - 운동

- 멋쟁이 거북이, 깊숙한 공부, 성장, 감사, 과정, 1.5

---
